{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI4Food Security South Africa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team EagleEyes:\n",
    "    \n",
    "    Christina Bukas, HMGU\n",
    "    Frauke Albrecht, DKRZ\n",
    "    Caroline Arnold, DKRZ\n",
    "\n",
    "_We would also like to thank Elisabeth Georgii from HMGU for her contribution in literature research and useful insights when discussing methods_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this challenge is to identify crop types in-season using Planet, Sentinel-1, and Sentinel-2 data. Field boundaries and cropy type labels are given as ground truth data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, all modules are imported that are necessary in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data from Planet, Sentinel-1, and Sentinel-2 satellites are given for a region in South Africa. We process the datasets individually, building on the routines that were provided through the starter notebook. The workflow is described here for Planet data.\n",
    "\n",
    "1. Rasterize the satellite images using the provided routines, for each field save a zip file containing the time series of bands and the mask \n",
    "2. Load the field ID and apply custom data transform. Available transforms are: spatial average, crop image, extract fixed number of pixels\n",
    "3. Save as `hdf5` files for use in training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full preprocessing module is included in `dataloaders/preprocessor.py`, with the dataset readers in `dataloaders/custom_{planet,sentinel_1,sentinel_2}_reader.py` and the image transforms in `dataloaders/custom_data_transform.py`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generated the training data by running\n",
    "\n",
    "`python preprocessor.py --region south-africa --data-source planet --t-random-extraction 640 --n-processes 64 --target-sub-dir extracted-640 --overwrite`\n",
    "\n",
    "`python preprocessor.py --region south-africa --data-source sentinel-1 --t-random-extraction 640 --n-processes 64 --target-sub-dir extracted-640 --overwrite`\n",
    "\n",
    "`python preprocessor.py --region south-africa --data-source sentinel-2 --t-random-extraction 640 --n-processes 64 --target-sub-dir extracted-640 --overwrite`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This extracts 640 pixels at random from a field, taking into account only the valid pixels as given by the mask. We resample if there are less than 640 pixels are available. Then, out of each field, we create 10 training samples, containing 64 extracted pixels each. This routine is execute in the notebook `DataInflation.ipynb`. Thus, we arrive at a total count of 41430 samples that can be used in training / validation. For the test set, 64 pixels are extracted at random per field.\n",
    "\n",
    "We performed this step after observing that on average there are about 900 valid pixels per crop in the Sentinel-1 dataset and 9700 valid pixels in the Planet dataset. Using samples much larger than 64 pixels would hinder the training routine in terms of memory, but by creating multiple samples per crop we take full advantage of the high resolution of our data, while ensuring a smooth training process. Indeed, with this 10x increase in our dataset we observed a great improvement in terms of performance of our models. On the test set, we only extract 64 pixels, i.e. one sample to predict the crop id."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use 10-fold cross validation, dividing the labeled data into training and validation set by field ID."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The crop types are not evenly distributed:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig_south_africa_crop_id_barplot.png\" alt=\"bar plot of crop types\" title=\"Available crop types for South Africa\" width=\"350\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the normalized vegetation index NDVI as the normalized difference of infrared and red planet data bands. Below (left) the average NDVI is shown throughout the growing season for the different crop types. For Sentinel-1 data, we calculate the Radar Vegetation Index (right)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig_south_africa_crop_id_ndvi.png\" alt=\"bar plot of crop types\" title=\"Available crop types for South Africa\" width=\"700\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSELTAE Model\n",
    "\n",
    "The model used is discribed in detail in \"Satellite Image Time Series Classification With Pixel-Set Encoders and Temporal Self-Attention\", Garnot. et al. (2020) and the code has been adapted from https://github.com/VSainteuf/pytorch-psetae. The model consists of a Spatial and a Temporal encoder. As spatial encoder a Pixel-Set Encoder is used, which uses as input a random set of pixels from each crop field. As Temporal Encoder, an attention based Neural Network is used. We train a combined model with Planet and Sentinel-1 data and therefore use two seperate PseLTae models for each dataset whose outputs are combined in the final decoder layer. Combination with an additional PseLTae model for Sentinel-2 data was explored, but did not improve the classification accuracy.\n",
    "\n",
    "In order to train the model the datasets have been created by the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import h5py\n",
    "import os\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from torch import randn\n",
    "import time\n",
    "from scipy.interpolate import splrep, splev\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "class EarthObservationDataset(Dataset):\n",
    "    '''\n",
    "    Parent class for Earth Observation Datasets\n",
    "\n",
    "    Preprocessed data is loaded from path:\n",
    "\n",
    "    -- in args namespace --\n",
    "      dev_data_dir    : file path on mistral, {germany, south africa}\n",
    "      input_data[0]   : data source {planet, sentinel-1, sentinel-2}\n",
    "      input_data_type : {extracted, extracted-640}\n",
    "      split           : {train, test}\n",
    "\n",
    "      If args.include_extras, the crop_area and crop_len are included for each sample\n",
    "    '''\n",
    "\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        \n",
    "        self.h5_file = h5py.File(os.path.join(args.dev_data_dir, args.input_data[0], args.input_data_type, f'{args.split}_data.h5'), 'r')\n",
    "\n",
    "        self.X = self.h5_file['image_stack'][:].astype(np.float32)\n",
    "        self.mask = self.h5_file['mask'][:].astype(bool)\n",
    "        self.fid = self.h5_file['fid'][:]\n",
    "        self.labels = self.h5_file['label'][:]\n",
    "        self.labels = self.labels - 1 # generated datafiles with classes from 1 ... k --> 0 ... k-1\n",
    "        if np.sum(np.isnan(self.X)) > 0:\n",
    "            print('WARNING: Filled NaNs and INFs with 0 in ', os.path.join(args.dev_data_dir, args.input_data[0], args.input_data_type, f'{args.split}_data.h5'))\n",
    "            self.X = np.nan_to_num(self.X, nan=0, posinf=0, neginf=0)\n",
    "\n",
    "        if args.include_extras:\n",
    "            labels_path = os.path.join(args.dev_data_dir,'labels_combined.geojson')\n",
    "            print('Adding extra features from ', labels_path)\n",
    "            extras = gpd.read_file(labels_path)\n",
    "\n",
    "            crop_area = []\n",
    "            crop_len  = []\n",
    "\n",
    "            extras_fid = extras[\"fid\"].values\n",
    "            extras_crop_area = extras[\"NORMALIZED_SHAPE_AREA\"].values\n",
    "            extras_crop_len = extras[\"NORMALIZED_SHAPE_LEN\"].values\n",
    "\n",
    "            for ii, ffid in enumerate(self.fid):\n",
    "                ix = np.where(extras_fid==ffid)[0][0]\n",
    "                crop_area.append(extras_crop_area[ix])\n",
    "                crop_len.append(extras_crop_len[ix])\n",
    "\n",
    "                if ii%(len(self.fid)//20) == 0:\n",
    "                    print(f'... finished {ii:8d}/{len(self.fid):8d} entries ({ii/len(self.fid)*100:.1f} %)')\n",
    "\n",
    "            self.extra_features = np.array([crop_area, crop_len]).T\n",
    "        else:\n",
    "            self.extra_features = None\n",
    "\n",
    "        # remove a fully masked sample from the Germany training data\n",
    "        if self.args.nr_classes == 9 and self.args.split == 'train':\n",
    "            if self.args.input_data_type == 'extracted':\n",
    "                bad_idx = [1225]\n",
    "            elif self.args.input_data_type == 'extracted-640':\n",
    "                bad_idx = [12250, 12251, 12252, 12253, 12254, 12255, 12256, 12257, 12258, 12259]\n",
    "            else:\n",
    "                bad_idx = []\n",
    "            self.X = np.delete(self.X, bad_idx, axis=0)\n",
    "            self.mask = np.delete(self.mask, bad_idx, axis=0)\n",
    "            self.fid = np.delete(self.fid, bad_idx, axis=0)\n",
    "            self.labels = np.delete(self.labels, bad_idx, axis=0)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels) \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = self.X[idx]\n",
    "        label = self.labels[idx]\n",
    "        mask = self.mask[idx]\n",
    "        fid = self.fid[idx]\n",
    "        if self.extra_features is not None:\n",
    "            extra_f = self.extra_features[idx]\n",
    "        else: extra_f = np.zeros_like(1)\n",
    "            \n",
    "        return (X, mask, fid, extra_f), label\n",
    "\n",
    "class Sentinel1Dataset(EarthObservationDataset):\n",
    "    '''\n",
    "    Sentinel 1 Dataset\n",
    "\n",
    "    If args.nri, calculates indices:\n",
    "\n",
    "    Normalized radar vegetation index (RVI)\n",
    "\n",
    "    If args.drop_channels_sentinel1, drop all bands\n",
    "\n",
    "    If args.savgol_filter, smooth the RVI with a Savitzky Golay filter\n",
    "\n",
    "    If args.split_nri, create two separate RVI channels for each observation angle\n",
    "    '''\n",
    "    def __init__(self, args):\n",
    "        super().__init__(args)\n",
    "        \n",
    "        self.X = self.X[:, :, :2, :] # only VV and VH (2) is the angle\n",
    "\n",
    "        # ! -- Germany test data has different length, cut to to train data length\n",
    "        if args.nr_classes == 9 and args.split == 'test':\n",
    "            print('Sentinel-1 shape before cut: ', self.X.shape)\n",
    "            self.X = self.X[:, 1:-1]\n",
    "            print('Sentinel-1 shape after cut (test set germany): ', self.X.shape)\n",
    "              \n",
    "        if args.nri:\n",
    "            nri = Sentinel1Dataset._calc_rvi(self.X, self.args.savgol_filter)\n",
    "            nri = np.expand_dims(nri, axis=2) # changed axis from 1 to 2\n",
    "            if args.drop_channels or args.drop_channels_sentinel1:\n",
    "                if args.split_nri: # split into two channels for different angles\n",
    "                    if self.args.savgol_filter:\n",
    "                        raise ValueError(\"Do not use Savitzky Golay filter and NRI split together\")\n",
    "                    nri_odd = nri[:, ::2]\n",
    "                    nri_even = nri[:, 1::2]\n",
    "                    nri_min_len = min(nri_odd.shape[1], nri_even.shape[1])\n",
    "                    self.X = np.concatenate([nri_odd[:, :nri_min_len], nri_even[:, :nri_min_len]], axis=2) \n",
    "                else:\n",
    "                    self.X = nri\n",
    "            else:\n",
    "                self.X = np.concatenate([self.X, nri], axis=2) \n",
    "\n",
    "        print('Final shape for Sentinel-1 image stack: ', self.X.shape)\n",
    "\n",
    "        '''\n",
    "        # normalization of datasets min-max\n",
    "        xmin=np.min(self.X, axis=(0,1,3))\n",
    "        xmax=np.max(self.X, axis=(0,1,3))\n",
    "        for i in range(self.X.shape[2]):\n",
    "            self.X[:,:,i,:] = (self.X[:,:,i,:] - xmin[i])/(xmax[i] - xmin[i])\n",
    "        '''\n",
    "                \n",
    "    @staticmethod\n",
    "    def _calc_rvi(X, rvi_filter=False):\n",
    "        VV = X[:,:,0,:]\n",
    "        VH = X[:,:,1,:]\n",
    "        dop = (VV/(VV+VH))\n",
    "        m = 1 - dop\n",
    "        radar_vegetation_index = (np.sqrt(dop))*((4*(VH))/(VV+VH))\n",
    "\n",
    "        eps = 1e-9 # avoid zero values\n",
    "        radar_vegetation_index = np.nan_to_num(radar_vegetation_index, nan=eps, posinf=eps, neginf=eps)\n",
    "\n",
    "        if not rvi_filter:\n",
    "            return radar_vegetation_index\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        print('Start to apply Savitzky Golay Filter to Sentinel-1 RVI')\n",
    "        for i in range(radar_vegetation_index.shape[0]):\n",
    "            for j in range(radar_vegetation_index.shape[-1]):\n",
    "                tmp = radar_vegetation_index[i, :, j]\n",
    "\n",
    "                smooth_rvi = savgol_filter(tmp, 15, 3)\n",
    "\n",
    "                radar_vegetation_index[i, :, j] = smooth_rvi\n",
    "\n",
    "        print(f'Applied Savitzky Golay Filter to Sentinel-1 RVI in {time.time() - start_time:.1f} seconds')\n",
    "\n",
    "        return radar_vegetation_index\n",
    "\n",
    "\n",
    "class PlanetDataset(EarthObservationDataset):\n",
    "    '''\n",
    "    Planet Dataset\n",
    "\n",
    "    If args.ndvi, calculates indices:\n",
    "\n",
    "    Normalized difference vegetation index (NDVI)\n",
    "\n",
    "    If args.drop_channels, drop all bands\n",
    "\n",
    "    '''\n",
    "\n",
    "    def __init__(self, args): \n",
    "        super().__init__(args)\n",
    "        if args.ndvi:\n",
    "            ndvi = PlanetDataset._calc_ndvi(self.X)\n",
    "            ndvi = np.expand_dims(ndvi, axis=2) # changed axis from 1 to 2\n",
    "            if args.drop_channels:\n",
    "                self.X = ndvi\n",
    "            else:\n",
    "                self.X = np.concatenate([self.X, ndvi], axis=2) # changed axis from 1 to 2\n",
    "\n",
    "        if args.nr_classes == 9 and args.vegetation_period: # Germany\n",
    "            ix_train_start = 83 # defined as the minimum of NDVI\n",
    "            ix_test_start  = 90 # such that NDVI maxima match\n",
    "            vg_length      = 180 # length of the vegetation period\n",
    "\n",
    "            if args.split == 'train':\n",
    "                self.X = self.X[:, ix_train_start:ix_train_start + vg_length]\n",
    "            elif args.split == 'test':\n",
    "                self.X = self.X[:, ix_test_start:ix_test_start + vg_length]\n",
    "\n",
    "        print('Final shape for Planet image stack', self.X.shape)\n",
    "\n",
    "        '''\n",
    "        # normalization of datasets min-max\n",
    "        xmin=np.min(self.X, axis=(0,1,3))\n",
    "        xmax=np.max(self.X, axis=(0,1,3))\n",
    "        for i in range(self.X.shape[2]):\n",
    "            self.X[:,:,i,:] = (self.X[:,:,i,:] - xmin[i])/(xmax[i] - xmin[i])\n",
    "        '''\n",
    "\n",
    "    @staticmethod\n",
    "    def _calc_ndvi(X):\n",
    "        '''\n",
    "        Calculate the normalized vegetation index\n",
    "\n",
    "        NDVI = (NIR - RED) / (NIR + RED)\n",
    "\n",
    "        '''\n",
    "        #print(X.shape) #(4143, 244, 4, 64)\n",
    "        nir = X[:, :, 3, :] # X[:, 3]\n",
    "        red = X[:, :, 2, :] # X[:, 2]\n",
    "        ndvi = (nir - red) / (nir + red)\n",
    "        ndvi = np.nan_to_num(ndvi)\n",
    "        return ndvi\n",
    "\n",
    "\n",
    "    \n",
    "class CombinedDataset(Dataset):\n",
    "    '''\n",
    "    Class for a combined dataset, holds PlanetDataset, Sentinel1Dataset, Sentinel2Dataset\n",
    "    as specified by args.input_data\n",
    "    '''\n",
    "    def __init__(self, args): \n",
    "        super().__init__()\n",
    "        self.datasets =[]\n",
    "        self.input_data = args.input_data.copy()\n",
    "        for input_data in self.input_data:\n",
    "            if input_data=='planet':\n",
    "                args.input_data = ['planet']\n",
    "                planet_dataset = PlanetDataset(args)\n",
    "                self.datasets.append(planet_dataset)\n",
    "            elif input_data=='planet-5':\n",
    "                args.input_data = ['planet-5']\n",
    "                planet5_dataset = PlanetDataset(args)\n",
    "                self.datasets.append(planet5_dataset)\n",
    "\n",
    "            elif input_data=='sentinel-1':\n",
    "                args.input_data = ['sentinel-1']\n",
    "                sentinel1_dataset = Sentinel1Dataset(args)\n",
    "                self.datasets.append(sentinel1_dataset)\n",
    "            elif input_data=='sentinel-2':\n",
    "                args.input_data = ['sentinel-2']\n",
    "                sentinel2_dataset = Sentinel2Dataset(args)\n",
    "                self.datasets.append(sentinel2_dataset)\n",
    "        args.input_data = self.input_data \n",
    "        for i in range(1, len(self.datasets)):\n",
    "            print('Assert dataset shape match', i-1, i)\n",
    "            assert (self.datasets[i-1].fid==self.datasets[i].fid).all(),'s1, s2 and/or planet not sorted correctly'\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.datasets[0].labels) \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return tuple(d[idx] for d in self.datasets)\n",
    "\n",
    "    \n",
    "class AddGaussianNoise(object):\n",
    "    '''\n",
    "    Add Gaussian noise to a sample\n",
    "    '''\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        return tensor + randn(tensor.size()) * self.std + self.mean\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following the model components are listed:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pixel-Set-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelSetEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, mlp1=[10, 32, 64], pooling='mean_std', mlp2=[64, 128], \n",
    "                 with_extra=True, extra_size=4):\n",
    "        \"\"\"\n",
    "        Pixel-set encoder.\n",
    "        Args:\n",
    "            input_dim (int): Number of channels of the input tensors\n",
    "            mlp1 (list):  Dimensions of the successive feature spaces of MLP1\n",
    "            pooling (str): Pixel-embedding pooling strategy, can be chosen in ('mean','std','max,'min')\n",
    "                or any underscore-separated combination thereof.\n",
    "            mlp2 (list): Dimensions of the successive feature spaces of MLP2\n",
    "            with_extra (bool): Whether additional pre-computed features are passed between the two MLPs\n",
    "            extra_size (int, optional): Number of channels of the additional features, if any.\n",
    "        \"\"\"\n",
    "        super(PixelSetEncoder, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.mlp1_dim = copy.deepcopy(mlp1)\n",
    "        self.mlp2_dim = copy.deepcopy(mlp2)\n",
    "        self.pooling = pooling\n",
    "\n",
    "        self.with_extra = with_extra\n",
    "        self.extra_size = extra_size\n",
    "        \n",
    "        self.name = 'PSE-{}-{}-{}'.format('|'.join(list(map(str, self.mlp1_dim))), pooling,\n",
    "                                          '|'.join(list(map(str, self.mlp2_dim))))\n",
    "\n",
    "        self.output_dim = input_dim * len(pooling.split('_')) if len(self.mlp2_dim) == 0 else self.mlp2_dim[-1]\n",
    "\n",
    "        inter_dim = self.mlp1_dim[-1] * len(pooling.split('_'))\n",
    "\n",
    "\n",
    "        if self.with_extra:\n",
    "            self.name += 'Extra'\n",
    "            inter_dim += self.extra_size\n",
    "\n",
    "        assert (input_dim == mlp1[0])\n",
    "        assert (inter_dim == mlp2[0])\n",
    "        # Feature extraction\n",
    "        layers = []\n",
    "        for i in range(len(self.mlp1_dim) - 1):\n",
    "            layers.append(linlayer(self.mlp1_dim[i], self.mlp1_dim[i + 1]))\n",
    "        self.mlp1 = nn.Sequential(*layers)\n",
    "\n",
    "        # MLP after pooling\n",
    "        layers = []\n",
    "        for i in range(len(self.mlp2_dim) - 1):\n",
    "            layers.append(nn.Linear(self.mlp2_dim[i], self.mlp2_dim[i + 1]))\n",
    "            layers.append(nn.BatchNorm1d(self.mlp2_dim[i + 1]))\n",
    "            if i < len(self.mlp2_dim) - 2:\n",
    "                layers.append(nn.ReLU())\n",
    "        self.mlp2 = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        The input of the PSE is a tuple of tensors as yielded by the PixelSetData class:\n",
    "          (Pixel-Set, Pixel-Mask) or ((Pixel-Set, Pixel-Mask), Extra-features)\n",
    "        Pixel-Set : Batch_size x (Sequence length) x Channel x Number of pixels\n",
    "        Pixel-Mask : Batch_size x (Sequence length) x Number of pixels\n",
    "        Extra-features : Batch_size x (Sequence length) x Number of features\n",
    "\n",
    "        If the input tensors have a temporal dimension, it will be combined with the batch dimension so that the\n",
    "        complete sequences are processed at once. Then the temporal dimension is separated back to produce a tensor of\n",
    "        shape Batch_size x Sequence length x Embedding dimension\n",
    "        \"\"\"\n",
    "        a, b = input\n",
    "        if len(a) == 2: # extra features included\n",
    "            out, mask = a\n",
    "            extra = b\n",
    "            if len(extra) == 2:\n",
    "                extra, bm = extra\n",
    "            extra = extra.unsqueeze(1).repeat(1, out.size(1) ,1).float()\n",
    "        else:\n",
    "            out, mask = a, b\n",
    "        mask = mask.unsqueeze(1).repeat(1, out.size(1) ,1)\n",
    "        \n",
    "        if len(out.shape) == 4:\n",
    "            # Combine batch and temporal dimensions in case of sequential input\n",
    "            reshape_needed = True\n",
    "            batch, temp = out.shape[:2]\n",
    "\n",
    "            out = out.view(batch * temp, *out.shape[2:])\n",
    "            mask = mask.view(batch * temp, -1)\n",
    "            if self.with_extra:\n",
    "                extra = extra.view(batch * temp, -1)\n",
    "        else:\n",
    "            reshape_needed = False\n",
    "        out = self.mlp1(out)\n",
    "        out = torch.cat([pooling_methods[n](out, mask) for n in self.pooling.split('_')], dim=1)\n",
    "\n",
    "        if self.with_extra:\n",
    "            out = torch.cat([out, extra], dim=1)\n",
    "        out = self.mlp2(out)\n",
    "        if reshape_needed:\n",
    "            out = out.view(batch, temp, -1)\n",
    "        return out\n",
    "\n",
    "class linlayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(linlayer, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        self.lin = nn.Linear(in_dim, out_dim)\n",
    "        self.bn = nn.BatchNorm1d(out_dim)\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = input.permute((0, 2, 1))  # to channel last\n",
    "        out = self.lin(out)\n",
    "\n",
    "        out = out.permute((0, 2, 1))  # to channel first\n",
    "        out = self.bn(out)\n",
    "        out = F.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "def masked_mean(x, mask):\n",
    "    out = x.permute((1, 0, 2))\n",
    "    out = out * mask\n",
    "    out = out.sum(dim=-1) / mask.sum(dim=-1)\n",
    "    out = out.permute((1, 0))\n",
    "    return out\n",
    "\n",
    "def masked_std(x, mask):\n",
    "    m = masked_mean(x, mask)\n",
    "\n",
    "    out = x.permute((2, 0, 1))\n",
    "    out = out - m\n",
    "    out = out.permute((2, 1, 0))\n",
    "\n",
    "    out = out * mask\n",
    "    d = mask.sum(dim=-1)\n",
    "    d[d == 1] = 2\n",
    "\n",
    "    out = (out ** 2).sum(dim=-1) / (d - 1)\n",
    "    out = torch.sqrt(out + 10e-32) # To ensure differentiability\n",
    "    out = out.permute(1, 0)\n",
    "    return out\n",
    "\n",
    "def maximum(x, mask):\n",
    "    return x.max(dim=-1)[0].squeeze()\n",
    "\n",
    "def minimum(x, mask):\n",
    "    return x.min(dim=-1)[0].squeeze()\n",
    "\n",
    "pooling_methods = {\n",
    "    'mean': masked_mean,\n",
    "    'std': masked_std,\n",
    "    'max': maximum,\n",
    "    'min': minimum\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lightweight Temporal Attention Encoder Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LTAE(nn.Module):\n",
    "    def __init__(self, in_channels=128, n_head=16, d_k=8, n_neurons=[256,128], dropout=0.2, \n",
    "                 d_model=256, T=1000, len_max_seq=24, positions=None, return_att=False):\n",
    "        \"\"\"\n",
    "        Sequence-to-embedding encoder.\n",
    "        Args:\n",
    "            in_channels (int): Number of channels of the input embeddings\n",
    "            n_head (int): Number of attention heads\n",
    "            d_k (int): Dimension of the key and query vectors\n",
    "            n_neurons (list): Defines the dimensions of the successive feature spaces of the MLP \n",
    "                that processes the concatenated outputs of the attention heads\n",
    "            dropout (float): dropout\n",
    "            T (int): Period to use for the positional encoding\n",
    "            len_max_seq (int, optional): Maximum sequence length, used to pre-compute the positional \n",
    "                encoding table\n",
    "            positions (list, optional): List of temporal positions to use instead of position \n",
    "                in the sequence\n",
    "            d_model (int, optional): If specified, the input tensors will first processed by a \n",
    "                fully connected layer to project them into a feature space of dimension d_model\n",
    "            return_att (bool): If true, the module returns the attention masks along with the \n",
    "                embeddings (default False)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        super(LTAE, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.positions = positions\n",
    "        self.n_neurons = copy.deepcopy(n_neurons)\n",
    "        self.return_att = return_att\n",
    "\n",
    "        if positions is None:\n",
    "            positions = len_max_seq + 1\n",
    "\n",
    "        if d_model is not None:\n",
    "            self.d_model = d_model\n",
    "            self.inconv = nn.Sequential(nn.Conv1d(in_channels, d_model, 1),\n",
    "                                        nn.LayerNorm((d_model, len_max_seq)))\n",
    "        else:\n",
    "            self.d_model = in_channels\n",
    "            self.inconv = None\n",
    "\n",
    "        sin_tab = get_sinusoid_encoding_table(positions, self.d_model // n_head, T=T)\n",
    "        self.position_enc = nn.Embedding.from_pretrained(torch.cat([sin_tab for _ in range(n_head)], dim=1),\n",
    "                                                         freeze=True)\n",
    "        self.inlayernorm = nn.LayerNorm(self.in_channels)\n",
    "        self.outlayernorm = nn.LayerNorm(n_neurons[-1])\n",
    "        self.attention_heads = MultiHeadAttention(n_head=n_head, d_k=d_k, d_in=self.d_model)\n",
    "\n",
    "        assert (self.n_neurons[0] == self.d_model)\n",
    "\n",
    "        activation = nn.ReLU()\n",
    "\n",
    "        layers = []\n",
    "        for i in range(len(self.n_neurons) - 1):\n",
    "            layers.extend([nn.Linear(self.n_neurons[i], self.n_neurons[i + 1]),\n",
    "                           nn.BatchNorm1d(self.n_neurons[i + 1]),\n",
    "                           activation])\n",
    "\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        sz_b, seq_len, d = x.shape\n",
    "        x = self.inlayernorm(x)\n",
    "\n",
    "        if self.inconv is not None:\n",
    "            x = self.inconv(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "\n",
    "        if self.positions is None:\n",
    "            src_pos = torch.arange(1, seq_len + 1, dtype=torch.long).expand(sz_b, seq_len).to(x.device)\n",
    "        else:\n",
    "            src_pos = torch.arange(0, seq_len, dtype=torch.long).expand(sz_b, seq_len).to(x.device)\n",
    "        enc_output = x + self.position_enc(src_pos)\n",
    "        enc_output, attn = self.attention_heads(enc_output, enc_output, enc_output)\n",
    "        enc_output = enc_output.permute(1, 0, 2).contiguous().view(sz_b, -1)  # Concatenate heads\n",
    "        enc_output = self.outlayernorm(self.dropout(self.mlp(enc_output)))\n",
    "\n",
    "        if self.return_att:\n",
    "            return enc_output, attn\n",
    "        else:\n",
    "            return enc_output\n",
    "\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    ''' Multi-Head Attention module '''\n",
    "\n",
    "    def __init__(self, n_head, d_k, d_in):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_k\n",
    "        self.d_in = d_in\n",
    "\n",
    "        self.Q = nn.Parameter(torch.zeros((n_head, d_k))).requires_grad_(True)\n",
    "        nn.init.normal_(self.Q, mean=0, std=np.sqrt(2.0 / (d_k)))\n",
    "\n",
    "        self.fc1_k = nn.Linear(d_in, n_head * d_k)\n",
    "        nn.init.normal_(self.fc1_k.weight, mean=0, std=np.sqrt(2.0 / (d_k)))\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(temperature=np.power(d_k, 0.5))\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        d_k, d_in, n_head = self.d_k, self.d_in, self.n_head\n",
    "        sz_b, seq_len, _ = q.size()\n",
    "\n",
    "        q = torch.stack([self.Q for _ in range(sz_b)], dim=1).view(-1, d_k)  # (n*b) x d_k\n",
    "\n",
    "        k = self.fc1_k(v).view(sz_b, seq_len, n_head, d_k)\n",
    "        k = k.permute(2, 0, 1, 3).contiguous().view(-1, seq_len, d_k)  # (n*b) x lk x dk\n",
    "\n",
    "        v = torch.stack(v.split(v.shape[-1] // n_head, dim=-1)).view(n_head * sz_b, seq_len, -1)\n",
    "        output, attn = self.attention(q, k, v)\n",
    "        attn = attn.view(n_head, sz_b, 1, seq_len)\n",
    "        attn = attn.squeeze(dim=2)\n",
    "\n",
    "        output = output.view(n_head, sz_b, 1, d_in // n_head)\n",
    "        output = output.squeeze(dim=2)\n",
    "\n",
    "        return output, attn\n",
    "\n",
    "    \n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    ''' Scaled Dot-Product Attention '''\n",
    "\n",
    "    def __init__(self, temperature, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        attn = torch.matmul(q.unsqueeze(1), k.transpose(1, 2))\n",
    "        attn = attn / self.temperature\n",
    "\n",
    "        attn = self.softmax(attn)\n",
    "        attn = self.dropout(attn)\n",
    "        output = torch.matmul(attn, v)\n",
    "\n",
    "        return output, attn\n",
    "\n",
    "    \n",
    "def get_sinusoid_encoding_table(positions, d_hid, T=1000):\n",
    "    ''' Sinusoid position encoding table\n",
    "    positions: int or list of integer, if int range(positions)'''\n",
    "\n",
    "    if isinstance(positions, int):\n",
    "        positions = list(range(positions))\n",
    "\n",
    "    def cal_angle(position, hid_idx):\n",
    "        return position / np.power(T, 2 * (hid_idx // 2) / d_hid)\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, hid_j) for hid_j in range(d_hid)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in positions])\n",
    "\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.FloatTensor(sinusoid_table).cuda()\n",
    "    else:\n",
    "        return torch.FloatTensor(sinusoid_table)\n",
    "\n",
    "def get_sinusoid_encoding_table_var(positions, d_hid, clip=4, offset=3, T=1000):\n",
    "    ''' Sinusoid position encoding table\n",
    "    positions: int or list of integer, if int range(positions)'''\n",
    "\n",
    "    if isinstance(positions, int):\n",
    "        positions = list(range(positions))\n",
    "\n",
    "    x = np.array(positions)\n",
    "\n",
    "    def cal_angle(position, hid_idx):\n",
    "        return position / np.power(T, 2 * (hid_idx + offset // 2) / d_hid)\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, hid_j) for hid_j in range(d_hid)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in positions])\n",
    "\n",
    "    sinusoid_table = np.sin(sinusoid_table)  # dim 2i\n",
    "    sinusoid_table[:, clip:] = torch.zeros(sinusoid_table[:, clip:].shape)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.FloatTensor(sinusoid_table).cuda()\n",
    "    else:\n",
    "        return torch.FloatTensor(sinusoid_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decoder(n_neurons):\n",
    "    \"\"\"Returns an MLP with the layer widths specified in n_neurons.\n",
    "    Every linear layer but the last one is followed by BatchNorm + ReLu\n",
    "\n",
    "    args:\n",
    "        n_neurons (list): List of int that specifies the width and length of the MLP.\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    for i in range(len(n_neurons)-1):\n",
    "        layers.append(nn.Linear(n_neurons[i], n_neurons[i+1]))\n",
    "        if i < (len(n_neurons) - 2):\n",
    "            layers.extend([\n",
    "                nn.BatchNorm1d(n_neurons[i + 1]),\n",
    "                nn.ReLU()\n",
    "            ])\n",
    "    m = nn.Sequential(*layers)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PseLTaeCombinedPlanetS1(nn.Module):\n",
    "    \"\"\"\n",
    "    Pixel-Set encoder + Lightweight Temporal Attention Encoder sequence classifier\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim_planet=5, input_dim_s1=3, mlp1_planet=[5, 32, 64], \n",
    "                 mlp1_s1=[3, 32, 64], pooling='mean_std', mlp2=[132, 128], with_extra=True,\n",
    "                 extra_size=4, n_head=16, d_k=8, d_model=256, mlp3_planet=[256, 128], \n",
    "                 mlp3_s1=[256, 64],dropout=0.2, T=1000, len_max_seq_planet=244, len_max_seq_s1=41,positions=None,\n",
    "                 mlp4=[128+64, 64, 32, 20], return_att=False):\n",
    "        super(PseLTaeCombinedPlanetS1, self).__init__()\n",
    "\n",
    "        # if extras is true then include it only in planet model\n",
    "        self.spatial_encoder_planet = PixelSetEncoder(input_dim_planet, mlp1=mlp1_planet, \n",
    "                                                      pooling=pooling, mlp2=mlp2, with_extra=with_extra,\n",
    "                                                      extra_size=extra_size)\n",
    "        self.temporal_encoder_planet = LTAE(in_channels=mlp2[-1], n_head=n_head, d_k=d_k,\n",
    "                                           d_model=d_model, n_neurons=mlp3_planet, dropout=dropout,\n",
    "                                           T=T, len_max_seq=len_max_seq_planet, positions=positions, \n",
    "                                           return_att=return_att)\n",
    "\n",
    "        mlp2_copy = mlp2.copy()\n",
    "        if with_extra: mlp2_copy[0] = mlp2_copy[0] - extra_size\n",
    "\n",
    "        self.spatial_encoder_s1 = PixelSetEncoder(input_dim_s1, mlp1=mlp1_s1, pooling=pooling, \n",
    "                                                  mlp2=mlp2_copy, with_extra=False,\n",
    "                                                  extra_size=extra_size)\n",
    "        self.temporal_encoder_s1 = LTAE(in_channels=mlp2_copy[-1], n_head=n_head, d_k=d_k,\n",
    "                                           d_model=d_model, n_neurons=mlp3_s1, dropout=dropout,\n",
    "                                           T=T, len_max_seq=len_max_seq_s1, positions=positions, \n",
    "                                           return_att=return_att)\n",
    "        self.decoder = get_decoder(mlp4)\n",
    "        self.return_att = return_att\n",
    "        self.param_ratio()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "         Args:\n",
    "            input(tuple): (Pixel-Set, Pixel-Mask) or ((Pixel-Set, Pixel-Mask), Extra-features)\n",
    "            Pixel-Set : Batch_size x Sequence length x Channel x Number of pixels\n",
    "            Pixel-Mask : Batch_size x Sequence length x Number of pixels\n",
    "            Extra-features : Batch_size x Sequence length x Number of features\n",
    "        \"\"\"\n",
    "\n",
    "        input1, input2 = input\n",
    "\n",
    "        out1 = self.spatial_encoder_planet(input1) # out size is 8,48,128\n",
    "        out2 = self.spatial_encoder_s1(input2)\n",
    "\n",
    "        if self.return_att:\n",
    "            out1, att1 = self.temporal_encoder_planet(out1)\n",
    "            out2, att2 = self.temporal_encoder_s1(out2)\n",
    "            out = torch.cat([out1, out2], dim=1)\n",
    "            out = self.decoder(out)\n",
    "            return out, (att1, att2)\n",
    "        else:\n",
    "            out1 = self.temporal_encoder_planet(out1)\n",
    "            out2 = self.temporal_encoder_s1(out2)\n",
    "            out = torch.cat([out1, out2], dim=1)\n",
    "            out = self.decoder(out)\n",
    "            return out\n",
    "\n",
    "    def param_ratio(self):\n",
    "        total = get_ntrainparams(self)\n",
    "        s = get_ntrainparams(self.spatial_encoder_planet)\n",
    "        t = get_ntrainparams(self.temporal_encoder_planet)\n",
    "        s1 = get_ntrainparams(self.spatial_encoder_s1)\n",
    "        t1 = get_ntrainparams(self.temporal_encoder_s1)\n",
    "        c = get_ntrainparams(self.decoder)\n",
    "\n",
    "        print('TOTAL TRAINABLE PARAMETERS : {}'.format(total))\n",
    "        print('PLANET Spatial {:5.1f}%'.format(s / total * 100))\n",
    "        print('PLANET Temporal {:5.1f}%'.format(t / total * 100))\n",
    "        print('SENTINEL-1 Spatial {:5.1f}%'.format(s1 / total * 100))\n",
    "        print('SENTINEL-1 Temporal {:5.1f}%'.format(t1 / total * 100))\n",
    "        print('Classifier {:5.1f}%'.format(c / total * 100))\n",
    "\n",
    "        return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "The hyperparameter optimization was included by us using NNI (https://nni.readthedocs.io/en/stable/). However, even with extensive hyperparameter optimization, the best results were achieved using the default network configuration. \n",
    "\n",
    "* General parameters\n",
    "    * batch_size: 8\n",
    "    * with_extra: False (i.e., do not include crop_area and crop_len)\n",
    "    * augmentation: False (no Gaussian random noise during training)\n",
    "* Pixel-Set Encoder module\n",
    "    * mlp1-in: 32, mlp1-out: 64 (Dimensions of the successive feature spaces of MLP1, Planet)\n",
    "    * mlp1-s1-in: 32,  mlp1-s1-out: 64 (Dimensions of the successive feature spaces of MLP1, Sentinel-1)\n",
    "    * factor: 16 (Scale for MLP3 input dimension)\n",
    "    * mlp3-out: 128 (MLP3 output dimension, Planet)\n",
    "    * mlp3-s1-out: 128 (MLP3 output dimension, Sentinel-1)\n",
    "* Temporal attention module\n",
    "    * n-head: 16 (attention heads)\n",
    "    * d-k: 8 (in temporal attention module)\n",
    "    * dropout: 0.2 \n",
    "* Decoder module\n",
    "    * mlp4-1: 64, mlp4-2: 64 (Dimensions of the successive feature spaces of MLP4)\n",
    "    * scale: 0.25 (sentinel-1 data weighted relative to planet data in the decoder layer of the combined model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Details\n",
    "\n",
    "The given dataset for training and validation was augmented as described in the preprocessing step, creating 10 samples out of each image. These samples were then randomly divided in training and validation set, however keeping (augmented) samples from the same field in either the training or validation set, in order to prevent data leakage. In total 90% of the data was used for training and 10% was kept for validation in each fold of the cross-validation.\n",
    "\n",
    "We use the Adam optimizer with the learning rate 0.001 and weight decay 0.000001. The training includes the monitoring of several metrics, which are calculated in  `./ai4food/evaluation_utils.py`. Accuracy was used as the early stopping condition with a patience of 10 epochs. \n",
    "\n",
    "As loss fuction, `focal loss` is used, which is not exactly equal to the evaluation metric the model is finally evaluated on, but is close enough for efficient training and is also used in Garnot. et al. (2020). The script is also copied to the Dockerimage. During training, both the training and the validation loss are close to zero, which indicates overfitting. This is countered with a $k$-fold validation scheme. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Credits to  github.com/clcarwin/focal_loss_pytorch\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=0, alpha=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha, (float, int)): self.alpha = torch.Tensor([alpha, 1 - alpha])\n",
    "        if isinstance(alpha, list): self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.dim() > 2:\n",
    "            input = input.view(input.size(0), input.size(1), -1)  # N,C,H,W => N,C,H*W\n",
    "            input = input.transpose(1, 2)  # N,C,H*W => N,H*W,C\n",
    "            input = input.contiguous().view(-1, input.size(2))  # N,H*W,C => N*H*W,C\n",
    "        target = target.view(-1, 1)\n",
    "\n",
    "        logpt = F.log_softmax(input, dim=1)\n",
    "        logpt = logpt.gather(1, target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = Variable(logpt.data.exp())\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type() != input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            at = self.alpha.gather(0, target.data.view(-1))\n",
    "            logpt = logpt * Variable(at)\n",
    "\n",
    "        loss = -1 * (1 - pt) ** self.gamma * logpt\n",
    "        if self.size_average:\n",
    "            return loss.mean()\n",
    "        else:\n",
    "            return loss.sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the loss computation, we set ```alpha``` as the inverse of the class frequencies in the training data, which were found during the exploration phase. In this way we aim to balance the class distribution.\n",
    "\n",
    "The training routing can be found in `./ai4food/training.py`. To reproduce our model training, execute\n",
    "\n",
    "```python3 training.py --num-workers 0 --input-data planet sentinel-1 --split train --ndvi 1 --nri 1 --input-dim 5 3 --include-extras 0 --dev-data-dir /swork/shared_data/2021-ai4food/dev_data/south-africa --nr-classes 5 --input-data-type extracted-640 --k-fold 10```\n",
    "\n",
    "Each model of the 10-fold cross validation, may be trained for a maximum of 100 epochs, but early stopping is applied, which stops the training routine after 10 epochs if no improvement to the validation accuracy is observed. Then the best performing model in terms of validation accuracy is saved.\n",
    "\n",
    "To run inference with the trained models on the test set and generate a submission file, execute\n",
    "\n",
    "```python3 training.py --num-workers 0 --input-data planet sentinel-1 --split test --ndvi 1 --nri 1 --input-dim 5 3 --include-extras 0 --save-preds --dev-data-dir /swork/shared_data/2021-ai4food/dev_data/south-africa --nr-classes 5 --input-data-type extracted-640 --k-fold 10```\n",
    "\n",
    "During inference all 10 models of the cross validation are used and an average of the probabilities of each model is obtained to get the final prediction for each sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crop fields colored with the predicted crop ID, as obtained from our best submission with a score of 3.60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig_south_africa_predictions_map.png\" alt=\"Predicted crop names for South Africa\" title=\"Predicted crop types for South Africa\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-4-food-security",
   "language": "python",
   "name": "ai-4-food-security"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
